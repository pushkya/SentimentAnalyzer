{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: fbprophet 0.6 requires cmdstanpy==0.4, which is not installed.\n",
      "ERROR: fbprophet 0.6 requires setuptools-git>=1.2, which is not installed.\n",
      "ERROR: Could not install packages due to an EnvironmentError: [WinError 5] Access is denied: 'c:\\\\users\\\\pushkar\\\\anaconda3\\\\lib\\\\site-packages\\\\~umpy\\\\core\\\\_multiarray_tests.cp37-win_amd64.pyd'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy==1.16.1\n",
      "  Downloading numpy-1.16.1-cp37-cp37m-win_amd64.whl (11.9 MB)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.18.1\n",
      "    Uninstalling numpy-1.18.1:\n",
      "      Successfully uninstalled numpy-1.18.1\n",
      "---review---\n",
      "[1, 2, 365, 1234, 5, 1156, 354, 11, 14, 2, 2, 7, 1016, 2, 2, 356, 44, 4, 1349, 500, 746, 5, 200, 4, 4132, 11, 2, 2, 1117, 1831, 2, 5, 4831, 26, 6, 2, 4183, 17, 369, 37, 215, 1345, 143, 2, 5, 1838, 8, 1974, 15, 36, 119, 257, 85, 52, 486, 9, 6, 2, 2, 63, 271, 6, 196, 96, 949, 4121, 4, 2, 7, 4, 2212, 2436, 819, 63, 47, 77, 2, 180, 6, 227, 11, 94, 2494, 2, 13, 423, 4, 168, 7, 4, 22, 5, 89, 665, 71, 270, 56, 5, 13, 197, 12, 161, 2, 99, 76, 23, 2, 7, 419, 665, 40, 91, 85, 108, 7, 4, 2084, 5, 4773, 81, 55, 52, 1901]\n",
      "---label---\n",
      "1\n",
      "Downloading data from https://s3.amazonaws.com/text-datasets/imdb_word_index.json\n",
      "1646592/1641221 [==============================] - 79s 48us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum review length: 2697\n",
      "Minimum review length: 70\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 500, 32)           160000    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 500, 32)           0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 213,301\n",
      "Trainable params: 213,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pushkar\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/30\n",
      "25000/25000 [==============================] - 261s 10ms/step - loss: 0.5132 - accuracy: 0.7549 - val_loss: 0.4022 - val_accuracy: 0.8263\n",
      "Epoch 2/30\n",
      "25000/25000 [==============================] - 559s 22ms/step - loss: 0.2993 - accuracy: 0.8799 - val_loss: 0.3021 - val_accuracy: 0.8752\n",
      "Epoch 3/30\n",
      "25000/25000 [==============================] - 1045s 42ms/step - loss: 0.2516 - accuracy: 0.9014 - val_loss: 0.3016 - val_accuracy: 0.8736\n",
      "Epoch 4/30\n",
      "25000/25000 [==============================] - 712s 28ms/step - loss: 0.2237 - accuracy: 0.9141 - val_loss: 0.3064 - val_accuracy: 0.8800\n",
      "Epoch 5/30\n",
      "25000/25000 [==============================] - 251s 10ms/step - loss: 0.2521 - accuracy: 0.8972 - val_loss: 0.3660 - val_accuracy: 0.8550\n",
      "Epoch 6/30\n",
      "25000/25000 [==============================] - 251s 10ms/step - loss: 0.2113 - accuracy: 0.9180 - val_loss: 0.3692 - val_accuracy: 0.8445\n",
      "Epoch 7/30\n",
      "25000/25000 [==============================] - 250s 10ms/step - loss: 0.1842 - accuracy: 0.9314 - val_loss: 0.3697 - val_accuracy: 0.8736\n",
      "Epoch 8/30\n",
      "25000/25000 [==============================] - 252s 10ms/step - loss: 0.1599 - accuracy: 0.9405 - val_loss: 0.3595 - val_accuracy: 0.8726\n",
      "Epoch 9/30\n",
      "25000/25000 [==============================] - 252s 10ms/step - loss: 0.1773 - accuracy: 0.9315 - val_loss: 0.3782 - val_accuracy: 0.8353\n",
      "Epoch 10/30\n",
      "25000/25000 [==============================] - 254s 10ms/step - loss: 0.1659 - accuracy: 0.9375 - val_loss: 0.3525 - val_accuracy: 0.8685\n",
      "Epoch 11/30\n",
      "25000/25000 [==============================] - 255s 10ms/step - loss: 0.1269 - accuracy: 0.9534 - val_loss: 0.3893 - val_accuracy: 0.8676\n",
      "Epoch 12/30\n",
      "25000/25000 [==============================] - 251s 10ms/step - loss: 0.1125 - accuracy: 0.9586 - val_loss: 0.3808 - val_accuracy: 0.8669\n",
      "Epoch 13/30\n",
      "25000/25000 [==============================] - 251s 10ms/step - loss: 0.0979 - accuracy: 0.9640 - val_loss: 0.4191 - val_accuracy: 0.8710\n",
      "Epoch 14/30\n",
      "25000/25000 [==============================] - 251s 10ms/step - loss: 0.0939 - accuracy: 0.9671 - val_loss: 0.4235 - val_accuracy: 0.8653\n",
      "Epoch 15/30\n",
      "25000/25000 [==============================] - 253s 10ms/step - loss: 0.0909 - accuracy: 0.9685 - val_loss: 0.5178 - val_accuracy: 0.8652\n",
      "Epoch 16/30\n",
      "25000/25000 [==============================] - 252s 10ms/step - loss: 0.0758 - accuracy: 0.9728 - val_loss: 0.5444 - val_accuracy: 0.8627\n",
      "Epoch 17/30\n",
      "25000/25000 [==============================] - 255s 10ms/step - loss: 0.1089 - accuracy: 0.9600 - val_loss: 0.5022 - val_accuracy: 0.8660\n",
      "Epoch 18/30\n",
      "25000/25000 [==============================] - 254s 10ms/step - loss: 0.0761 - accuracy: 0.9733 - val_loss: 0.4755 - val_accuracy: 0.8650\n",
      "Epoch 19/30\n",
      "25000/25000 [==============================] - 253s 10ms/step - loss: 0.0629 - accuracy: 0.9794 - val_loss: 0.5356 - val_accuracy: 0.8674\n",
      "Epoch 20/30\n",
      "25000/25000 [==============================] - 257s 10ms/step - loss: 0.0781 - accuracy: 0.9746 - val_loss: 0.5437 - val_accuracy: 0.8654\n",
      "Epoch 21/30\n",
      "25000/25000 [==============================] - 259s 10ms/step - loss: 0.0760 - accuracy: 0.9742 - val_loss: 0.5004 - val_accuracy: 0.8543\n",
      "Epoch 22/30\n",
      "25000/25000 [==============================] - 268s 11ms/step - loss: 0.0969 - accuracy: 0.9668 - val_loss: 0.4633 - val_accuracy: 0.8638\n",
      "Epoch 23/30\n",
      "25000/25000 [==============================] - 269s 11ms/step - loss: 0.0627 - accuracy: 0.9783 - val_loss: 0.5272 - val_accuracy: 0.8672\n",
      "Epoch 24/30\n",
      "25000/25000 [==============================] - 268s 11ms/step - loss: 0.0510 - accuracy: 0.9831 - val_loss: 0.6120 - val_accuracy: 0.8666\n",
      "Epoch 25/30\n",
      "25000/25000 [==============================] - 263s 11ms/step - loss: 0.0450 - accuracy: 0.9859 - val_loss: 0.6748 - val_accuracy: 0.8556\n",
      "Epoch 26/30\n",
      "25000/25000 [==============================] - 268s 11ms/step - loss: 0.0433 - accuracy: 0.9866 - val_loss: 0.6388 - val_accuracy: 0.8642\n",
      "Epoch 27/30\n",
      "25000/25000 [==============================] - 266s 11ms/step - loss: 0.0389 - accuracy: 0.9879 - val_loss: 0.5993 - val_accuracy: 0.8656\n",
      "Epoch 28/30\n",
      "25000/25000 [==============================] - 269s 11ms/step - loss: 0.0553 - accuracy: 0.9812 - val_loss: 0.6133 - val_accuracy: 0.8604\n",
      "Epoch 29/30\n",
      "25000/25000 [==============================] - 270s 11ms/step - loss: 0.0448 - accuracy: 0.9857 - val_loss: 0.6304 - val_accuracy: 0.8586\n",
      "Epoch 30/30\n",
      "25000/25000 [==============================] - 269s 11ms/step - loss: 0.0531 - accuracy: 0.9821 - val_loss: 0.6256 - val_accuracy: 0.8634\n",
      "Accuracy: 86.34%\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy==1.16.1\n",
    "\n",
    "#Import Libraries\n",
    "import numpy\n",
    "from numpy import array\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import load_model\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)\n",
    "\n",
    "# Load the dataset but only keep the top n words and zero out the rest i.e keep vocabulary size as 5000\n",
    "top_words = 5000 #vocabulary_size = 5000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\n",
    "\n",
    "'''Inspect a sample review and its label.Note that the review is stored as a sequence of integers. These are word IDs that \n",
    "have been pre-assigned to individual words, and the label is an integer (0 for negative, 1 for positive).'''\n",
    "print('---review---')\n",
    "print(X_train[6])\n",
    "print('---label---')\n",
    "print(y_train[6])\n",
    "\n",
    "'''We can use the dictionary returned by imdb.get_word_index() to map the review back to the original words.'''\n",
    "word2id = imdb.get_word_index()\n",
    "id2word = {i: word for word, i in word2id.items()}\n",
    "print('---review with words---')\n",
    "print([id2word.get(i, ' ') for i in X_train[6]])\n",
    "print('---label---')\n",
    "print(y_train[6])\n",
    "\n",
    "print(word2id)\n",
    "\n",
    "print(id2word)\n",
    "\n",
    "#Maximum review length and minimum review length.\n",
    "print('Maximum review length: {}'.format(\n",
    "len(max((X_train + X_test), key=len))))\n",
    "\n",
    "print('Minimum review length: {}'.format(\n",
    "len(min((X_train + X_test), key=len))))\n",
    "\n",
    "'''In order to feed this data into our RNN, all input documents must have the same length. We will limit the maximum review length to maximum words by truncating \n",
    "longer reviews and padding shorter reviews with a null value (0). We can accomplish this task using the pad_sequences() function in Keras. Here, setting max_review_length \n",
    "to 500.'''\n",
    "\n",
    "max_review_length = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)\n",
    "\n",
    "'''Remember that our input is a sequence of words (technically, integer word IDs) of maximum length = max_review_length, and our output is a binary sentiment \n",
    "label (0 or 1).\n",
    "'''\n",
    "# create the model\n",
    "embedding_vecor_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "'''We first need to compile our model by specifying the loss function and optimizer we want to use while training, as well as any evaluation metrics \n",
    "we’d like to measure. Specify the appropriate parameters, including at least one metric ‘accuracy’.'''\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=30, batch_size=64)\n",
    "\n",
    "#Calculate Accuracy\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
